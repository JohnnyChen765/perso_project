{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Exercice de formatage de date sur différents modèles d'encodeur"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input                    Target                   \n--------------------------------------------------\nSeptember 20, 7075       7075-09-20               \nMay 15, 8579             8579-05-15               \nJanuary 11, 7103         7103-01-11               \n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "\n",
    "# cannot use strftime()'s %B format since it depends on the locale\n",
    "MONTHS = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n",
    "          \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
    "\n",
    "def random_dates(n_dates):\n",
    "    min_date = date(1000, 1, 1).toordinal()\n",
    "    max_date = date(9999, 12, 31).toordinal()\n",
    "\n",
    "    ordinals = np.random.randint(max_date - min_date, size=n_dates) + min_date\n",
    "    dates = [date.fromordinal(ordinal) for ordinal in ordinals]\n",
    "\n",
    "    x = [MONTHS[dt.month - 1] + \" \" + dt.strftime(\"%d, %Y\") for dt in dates]\n",
    "    y = [dt.isoformat() for dt in dates]\n",
    "    return x, y\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "n_dates = 3\n",
    "x_example, y_example = random_dates(n_dates)\n",
    "print(\"{:25s}{:25s}\".format(\"Input\", \"Target\"))\n",
    "print(\"-\" * 50)\n",
    "for idx in range(n_dates):\n",
    "    print(\"{:25s}{:25s}\".format(x_example[idx], y_example[idx]))\n",
    "\n",
    "# Liste de tous les char possibles en entrée et en sortie.\n",
    "INPUT_CHARS = \"\".join(sorted(set(\"\".join(MONTHS)))) + \"0123456789, \"\n",
    "#'ADFJMNOSabceghilmnoprstuvy01234567890, '\n",
    "\n",
    "OUTPUT_CHARS = \"0123456789-\"\n",
    "\n",
    "# Ce que nous avons besoin de faire:\n",
    "# - nous devons attribuer à chaque char un identifiant.\n",
    "# - puis pour chaque \"phrase\", le tokenizer, ici ça sera simplement récupérer chaque caractère\n",
    "# - On train un modèle dessus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "words: [b'A' b'D' b'F' b'J' b'M' b'N' b'O' b'S' b'a' b'b' b'c' b'e' b'g' b'h'\n b'i' b'l' b'm' b'n' b'o' b'p' b'r' b's' b't' b'u' b'v' b'y' b'0' b'1'\n b'2' b'3' b'4' b'5' b'6' b'7' b'8' b'9' b',' b' ']\nword_ids: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n 24 25 26 27 28 29 30 31 32 33 34 35 36 37]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(18,), dtype=int64, numpy=\n",
       "array([ 7, 11, 19, 22, 11, 16,  9, 11, 20, 37, 28, 26, 36, 37, 33, 26, 33,\n",
       "       31])>"
      ]
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "source": [
    "# Pour créer une table de lookup, mais que si le vocabulaire est au niveau du mot\n",
    "def create_lookup_table(phrases): # phrases est une liste de liste de mots. Autrement dit, les mots doivent déjà être tokenizé\n",
    "    # Créer la table de lookup\n",
    "    words = tf.constant(phrases)\n",
    "    word_ids = tf.range(len(phrases), dtype=tf.int64)\n",
    "    print(f\"words: {words}\")\n",
    "    print(f\"word_ids: {word_ids}\")\n",
    "\n",
    "    num_oov_buckets = 1000\n",
    "    vocab_init = tf.lookup.KeyValueTensorInitializer(keys=words, values=word_ids)\n",
    "    encoder_table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)\n",
    "\n",
    "    def encode_words(X_batch):\n",
    "        return encoder_table.lookup(X_batch)\n",
    "\n",
    "    return encoder_table, encode_words, words, word_ids\n",
    "\n",
    "def split_string(string):\n",
    "    return [s for s in string]\n",
    "    \n",
    "# decoder_vocab_init = tf.lookup.KeyValueTensorInitializer(keys=word_ids, values=words)\n",
    "# decoder_table = tf.lookup.StaticVocabularyTable(decoder_vocab_init, num_oov_buckets)\n",
    "\n",
    "\n",
    "# def decode_words(X_batch):\n",
    "#     return decoder_vocab_init.lookup(X_batch)\n",
    "\n",
    "encoder_table, encode_words, _, _ = create_lookup_table([char for char in INPUT_CHARS])\n",
    "encode_words(tf.constant(split_string('September 20, 7075')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "max_id:34\ndataset_size:2\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[5, 21, 26, 28, 21, 2, 19, 21, 27, 33, 8, 6, 32, 33, 13, 6, 13, 11],\n",
       " [5, 21, 26, 28, 21, 2, 19, 21, 27, 33, 8, 6, 32, 33, 13, 6, 13, 12]]"
      ]
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True, split='') # we want the space as a char too\n",
    "tokenizer.fit_on_texts([INPUT_CHARS, OUTPUT_CHARS])\n",
    "max_id = len(tokenizer.word_index) # number of distinct characters\n",
    "dataset_size = tokenizer.document_count\n",
    "print(f\"max_id:{max_id}\")\n",
    "print(f\"dataset_size:{dataset_size}\")\n",
    "\n",
    "def tokenize_encode_lines(lines):\n",
    "    return tokenizer.texts_to_sequences(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "((<tf.Tensor: shape=(18,), dtype=int32, numpy=\narray([ 5, 21, 26, 28, 21,  2, 19, 21, 27, 33,  8,  6, 32, 33, 13,  6, 13,\n       11], dtype=int32)>, <tf.Tensor: shape=(11,), dtype=int32, numpy=array([ 0, 13,  6, 13, 11, 34,  6, 15, 34,  8,  6], dtype=int32)>), <tf.Tensor: shape=(10,), dtype=int32, numpy=array([13,  6, 13, 11, 34,  6, 15, 34,  8,  6], dtype=int32)>)\nCount:1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def create_dataset(n_dates):\n",
    "    x, y = random_dates(n_dates)\n",
    "    return x, y\n",
    "\n",
    "def shifted_output_sequences(Y):\n",
    "    sos_id = 0\n",
    "    sos_tokens = tf.fill(dims=(1,), value=sos_id)\n",
    "    return tf.concat([sos_tokens, Y], axis=0)\n",
    "\n",
    "def disp_dataset(ds, limit=None, print_elements=True):\n",
    "    count = 0\n",
    "    for x in ds:\n",
    "        count += 1\n",
    "        if print_elements:\n",
    "            print(x)\n",
    "        if limit is not None and count >= limit:\n",
    "            break\n",
    "    print(f\"Count:{count}\")\n",
    "\n",
    "dataset = create_dataset(10000) # return X de taille 10000 et Y de taille 10000\n",
    "# On transforme les phrases en liste d'id\n",
    "dataset = (tokenize_encode_lines(dataset[0]), tokenize_encode_lines(dataset[1]))\n",
    "# On transforme la liste en tenseurs. comme les phrases n'ont pas toutes la même longeueur, on rajoute un padding d'id 0: c'est l'idée du ragged.constant.to_tensor()\n",
    "dataset = (tf.ragged.constant(dataset[0]).to_tensor(), tf.ragged.constant(dataset[1]).to_tensor())\n",
    "dataset = tf.data.Dataset.from_tensor_slices(dataset)\n",
    "dataset = dataset.map(lambda x,y: ((x, shifted_output_sequences(y)),y))\n",
    "# Nous aurons besoin de 2 inputs : 1 pour l'encodeur, et l'autre pour le décodeur, qui n'est que la cible y, où on met un token d'id 0 au début pour shifter. On choisit le token d'id 0 arbitrairement, car nous n'avons pas de token <sos> ou <eos>, mais un simple 0 pour essayer d'indiquer l'inimportance.\n",
    "disp_dataset(dataset, limit=1)\n",
    "\n",
    "test_set = dataset.take(1000)\n",
    "train_set = dataset.skip(1000)\n",
    "valid_set = train_set.take(1000)\n",
    "train_set = train_set.skip(1000)\n",
    "\n",
    "train_set = train_set.batch(32).prefetch(1)\n",
    "valid_set = valid_set.batch(32).prefetch(1)\n",
    "test_set = test_set.batch(32).prefetch(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "InvalidArgumentError",
     "evalue": "2 root error(s) found.\n  (0) Invalid argument:  Incompatible shapes: [32,10] vs. [32,11]\n\t [[node Equal (defined at <ipython-input-143-9d3f51f4b3b6>:48) ]]\n\t [[gradient_tape/model_4/embedding_8/embedding_lookup/Reshape/_136]]\n  (1) Invalid argument:  Incompatible shapes: [32,10] vs. [32,11]\n\t [[node Equal (defined at <ipython-input-143-9d3f51f4b3b6>:48) ]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_21103]\n\nFunction call stack:\ntrain_function -> train_function\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-143-9d3f51f4b3b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m               metrics=[\"accuracy\"])\n\u001b[1;32m     47\u001b[0m history = model.fit(train_set, epochs=15,\n\u001b[0;32m---> 48\u001b[0;31m                     validation_data=valid_set)\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: 2 root error(s) found.\n  (0) Invalid argument:  Incompatible shapes: [32,10] vs. [32,11]\n\t [[node Equal (defined at <ipython-input-143-9d3f51f4b3b6>:48) ]]\n\t [[gradient_tape/model_4/embedding_8/embedding_lookup/Reshape/_136]]\n  (1) Invalid argument:  Incompatible shapes: [32,10] vs. [32,11]\n\t [[node Equal (defined at <ipython-input-143-9d3f51f4b3b6>:48) ]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_21103]\n\nFunction call stack:\ntrain_function -> train_function\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "def create_simple_model():\n",
    "    encoder_embedding_size = 32\n",
    "    decoder_embedding_size = 32\n",
    "    units = 128\n",
    "\n",
    "    encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "    decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "    sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n",
    "\n",
    "    vocab_size = len(INPUT_CHARS) + 1 # on prend en compte le token d'id 0 qui est le padding\n",
    "    embeddings = keras.layers.Embedding(len(INPUT_CHARS) + 1, encoder_embedding_size)\n",
    "    encoder_embeddings = embeddings(encoder_inputs)\n",
    "    decoder_embeddings = embeddings(decoder_inputs)\n",
    "\n",
    "    encoder = keras.layers.LSTM(units)\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
    "    encoder_state = [state_h, state_c]\n",
    "\n",
    "    sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "    decoder_cell = keras.layers.LSTMCell(units)\n",
    "    output_layer = keras.layers.Dense(vocab_size)\n",
    "\n",
    "    decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell,\n",
    "                                                    sampler,\n",
    "                                                    output_layer=output_layer)\n",
    "    final_outputs, final_state, final_sequence_lengths = decoder(\n",
    "        decoder_embeddings,\n",
    "        initial_state=encoder_state,\n",
    "        sequence_lengths=sequence_lengths)\n",
    "\n",
    "    Y_proba =tf.nn.softmax(final_outputs.rnn_outputs)\n",
    "    model = keras.models.Model(inputs=[encoder_inputs, decoder_inputs],\n",
    "                            outputs=[Y_proba])\n",
    "    return model\n",
    "\n",
    "model = create_simple_model()\n",
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, epochs=15,\n",
    "                    validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 2, 3, 4], dtype=int32)>"
      ]
     },
     "metadata": {},
     "execution_count": 134
    }
   ],
   "source": [
    "Y = tf.constant([2,3,4])\n",
    "sos_tokens = tf.fill(dims=(1,), value=0)\n",
    "sos_tokens\n",
    "tf.concat([sos_tokens, Y], axis=0)"
   ]
  }
 ]
}